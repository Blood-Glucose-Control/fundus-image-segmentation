{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fundus Image Segmentation - Google Colab Notebook\n",
        "\n",
        "This notebook provides a comprehensive solution for fundus image segmentation to predict the percentage of affected areas in retinal images.\n",
        "\n",
        "## Features\n",
        "- U-Net based segmentation model\n",
        "- Comprehensive data augmentation\n",
        "- Advanced training with combined loss (BCE + Dice)\n",
        "- Detailed evaluation metrics\n",
        "- Percentage calculation of affected areas\n",
        "\n",
        "## References\n",
        "- Deep learning for diabetic retinopathy detection: https://jamanetwork.com/journals/jama/fullarticle/2588763\n",
        "- U-Net: https://arxiv.org/abs/1610.02391\n",
        "- AI-based retinal analysis: https://www.nature.com/articles/s41467-021-23458-5\n"
      ],
      "metadata": {
        "id": "header_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Blood-Glucose-Control/fundus-image-segmentation.git\n",
        "%cd fundus-image-segmentation\n",
        "\n",
        "# Install required packages\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install additional packages for Colab\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install albumentations\n",
        "!pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import our modules\n",
        "from models import UNet\n",
        "from data import FundusDataset, create_data_loaders, get_train_transform, get_val_transform\n",
        "from training import Trainer\n",
        "from evaluation import ModelEvaluator\n",
        "from utils import visualize_segmentation, calculate_affected_percentage, create_sample_dataset_structure\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
      ],
      "metadata": {
        "id": "imports_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Setup\n",
        "\n",
        "You can either:\n",
        "1. Upload your own dataset\n",
        "2. Use a publicly available dataset like DRIVE, STARE, or IDRiD\n",
        "3. Create synthetic data for testing"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset structure\n",
        "from utils import create_sample_dataset_structure\n",
        "create_sample_dataset_structure('dataset')\n",
        "\n",
        "print(\"Dataset structure created!\")\n",
        "print(\"Please upload your images and masks to the appropriate directories.\")\n",
        "print(\"\\nDataset structure:\")\n",
        "!find dataset -type d | sort"
      ],
      "metadata": {
        "id": "dataset_setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option: Download sample dataset (uncomment if needed)\n",
        "# !wget -O sample_data.zip \"YOUR_DATASET_DOWNLOAD_LINK\"\n",
        "# !unzip -q sample_data.zip\n",
        "# !mv sample_data/* dataset/\n",
        "\n",
        "# For demonstration, let's create synthetic data\n",
        "def create_synthetic_fundus_data(num_samples=50, image_size=512):\n",
        "    \"\"\"Create synthetic fundus images and masks for demonstration\"\"\"\n",
        "    \n",
        "    for split in ['train', 'val']:\n",
        "        n_samples = num_samples if split == 'train' else num_samples // 5\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            # Create synthetic fundus-like image\n",
        "            image = np.random.randint(0, 255, (image_size, image_size, 3), dtype=np.uint8)\n",
        "            \n",
        "            # Add circular fundus boundary\n",
        "            center = (image_size // 2, image_size // 2)\n",
        "            radius = image_size // 3\n",
        "            Y, X = np.ogrid[:image_size, :image_size]\n",
        "            dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
        "            circle_mask = dist_from_center <= radius\n",
        "            \n",
        "            # Apply circular mask to image\n",
        "            image[~circle_mask] = 0\n",
        "            \n",
        "            # Create synthetic mask with some affected regions\n",
        "            mask = np.zeros((image_size, image_size), dtype=np.uint8)\n",
        "            \n",
        "            # Add random lesions\n",
        "            num_lesions = np.random.randint(0, 5)\n",
        "            for _ in range(num_lesions):\n",
        "                lesion_center = (np.random.randint(50, image_size-50), \n",
        "                               np.random.randint(50, image_size-50))\n",
        "                lesion_radius = np.random.randint(10, 50)\n",
        "                \n",
        "                Y, X = np.ogrid[:image_size, :image_size]\n",
        "                lesion_dist = np.sqrt((X - lesion_center[0])**2 + (Y - lesion_center[1])**2)\n",
        "                lesion_mask = lesion_dist <= lesion_radius\n",
        "                \n",
        "                mask[lesion_mask & circle_mask] = 255\n",
        "            \n",
        "            # Save image and mask\n",
        "            img_path = f'dataset/{split}/images/synthetic_{i:03d}.png'\n",
        "            mask_path = f'dataset/{split}/masks/synthetic_{i:03d}.png'\n",
        "            \n",
        "            cv2.imwrite(img_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "            cv2.imwrite(mask_path, mask)\n",
        "    \n",
        "    print(f\"Created {num_samples} training and {num_samples//5} validation synthetic samples\")\n",
        "\n",
        "# Create synthetic data for demonstration\n",
        "create_synthetic_fundus_data(num_samples=100)\n",
        "\n",
        "# Check dataset\n",
        "train_images = len(os.listdir('dataset/train/images'))\n",
        "train_masks = len(os.listdir('dataset/train/masks'))\n",
        "val_images = len(os.listdir('dataset/val/images'))\n",
        "val_masks = len(os.listdir('dataset/val/masks'))\n",
        "\n",
        "print(f\"Training: {train_images} images, {train_masks} masks\")\n",
        "print(f\"Validation: {val_images} images, {val_masks} masks\")"
      ],
      "metadata": {
        "id": "synthetic_data_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample data\n",
        "import random\n",
        "\n",
        "# Load a random sample\n",
        "train_images = os.listdir('dataset/train/images')\n",
        "sample_img = random.choice(train_images)\n",
        "\n",
        "img_path = f'dataset/train/images/{sample_img}'\n",
        "mask_path = f'dataset/train/masks/{sample_img}'\n",
        "\n",
        "image = cv2.imread(img_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Visualize\n",
        "visualize_segmentation(image, mask, title=f\"Sample: {sample_img}\")\n",
        "\n",
        "# Calculate percentage\n",
        "percentage = calculate_affected_percentage(mask)\n",
        "print(f\"Affected percentage in sample: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "visualize_sample_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "BATCH_SIZE = 4\n",
        "IMAGE_SIZE = 512\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 20  # Reduced for demo, increase for real training\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Training on: {DEVICE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Image size: {IMAGE_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Number of epochs: {NUM_EPOCHS}\")"
      ],
      "metadata": {
        "id": "training_params_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader, val_loader = create_data_loaders(\n",
        "    'dataset/train/images',\n",
        "    'dataset/train/masks',\n",
        "    'dataset/val/images',\n",
        "    'dataset/val/masks',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Test data loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"Sample batch - Images shape: {sample_batch[0].shape}\")\n",
        "print(f\"Sample batch - Masks shape: {sample_batch[1].shape}\")"
      ],
      "metadata": {
        "id": "create_loaders_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and initialize model\n",
        "model = UNet(n_channels=3, n_classes=2, bilinear=True)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=DEVICE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    log_dir='logs'\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ],
      "metadata": {
        "id": "create_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train(num_epochs=NUM_EPOCHS, save_dir='checkpoints')\n",
        "\n",
        "# Plot training history\n",
        "trainer.plot_training_history('training_history.png')\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "train_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "evaluation_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model for evaluation\n",
        "best_model_path = 'checkpoints/best_model.pth'\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Best model loaded for evaluation\")\n",
        "    \n",
        "    # Print training metrics\n",
        "    print(f\"Best model metrics:\")\n",
        "    print(f\"  Training loss: {checkpoint.get('train_loss', 'N/A'):.4f}\")\n",
        "    print(f\"  Validation loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
        "    if 'val_metrics' in checkpoint:\n",
        "        metrics = checkpoint['val_metrics']\n",
        "        print(f\"  IoU: {metrics.get('iou', 'N/A'):.4f}\")\n",
        "        print(f\"  Dice: {metrics.get('dice', 'N/A'):.4f}\")\n",
        "        print(f\"  Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\")\nelse:\n",
        "    print(\"No saved model found, using current model state\")"
      ],
      "metadata": {
        "id": "load_best_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluator and run evaluation on validation set\n",
        "evaluator = ModelEvaluator(model, DEVICE)\n",
        "\n",
        "print(\"Running evaluation on validation set...\")\n",
        "summary, detailed_metrics = evaluator.evaluate_model(\n",
        "    val_loader, \n",
        "    threshold=0.5, \n",
        "    save_results=True, \n",
        "    output_dir='evaluation_results'\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for metric, stats in summary.items():\n",
        "    if isinstance(stats, dict):\n",
        "        print(f\"{metric.capitalize()}:\")\n",
        "        print(f\"  Mean: {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
        "        print(f\"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
        "        print(f\"  Median: {stats['median']:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "evaluation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Inference on Sample Images"
      ],
      "metadata": {
        "id": "inference_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for single image inference\n",
        "def predict_fundus_image(model, image_path, device, image_size=512, threshold=0.5):\n",
        "    \"\"\"Predict segmentation and percentage for a single image\"\"\"\n",
        "    from data import preprocess_single_image\n",
        "    \n",
        "    # Preprocess image\n",
        "    image_tensor = preprocess_single_image(image_path, image_size)\n",
        "    image_tensor = image_tensor.to(device)\n",
        "    \n",
        "    # Load original for visualization\n",
        "    original = cv2.imread(image_path)\n",
        "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "    original = cv2.resize(original, (image_size, image_size))\n",
        "    \n",
        "    # Run inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        pred_mask = (probs[0, 1, :, :] > threshold).float().cpu().numpy()\n",
        "    \n",
        "    # Calculate percentage\n",
        "    percentage = calculate_affected_percentage(pred_mask, threshold)\n",
        "    \n",
        "    return original, pred_mask, percentage\n",
        "\n",
        "# Test on a few validation images\n",
        "val_images = os.listdir('dataset/val/images')[:3]  # Take first 3 images\n",
        "\n",
        "for img_name in val_images:\n",
        "    img_path = f'dataset/val/images/{img_name}'\n",
        "    mask_path = f'dataset/val/masks/{img_name}'\n",
        "    \n",
        "    # Load ground truth mask\n",
        "    if os.path.exists(mask_path):\n",
        "        gt_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        gt_mask = cv2.resize(gt_mask, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "        gt_percentage = calculate_affected_percentage(gt_mask)\n",
        "    else:\n",
        "        gt_mask = None\n",
        "        gt_percentage = 0\n",
        "    \n",
        "    # Make prediction\n",
        "    original, pred_mask, pred_percentage = predict_fundus_image(\n",
        "        model, img_path, DEVICE, IMAGE_SIZE\n",
        "    )\n",
        "    \n",
        "    # Visualize results\n",
        "    if gt_mask is not None:\n",
        "        visualize_segmentation(\n",
        "            original, gt_mask/255.0, pred_mask, \n",
        "            title=f\"{img_name} - GT: {gt_percentage:.1f}%, Pred: {pred_percentage:.1f}%\"\n",
        "        )\n",
        "    else:\n",
        "        visualize_segmentation(\n",
        "            original, pred_mask, \n",
        "            title=f\"{img_name} - Predicted: {pred_percentage:.1f}% affected\"\n",
        "        )\n",
        "    \n",
        "    print(f\"Image: {img_name}\")\n",
        "    if gt_mask is not None:\n",
        "        print(f\"  Ground Truth: {gt_percentage:.2f}% affected\")\n",
        "        print(f\"  Prediction: {pred_percentage:.2f}% affected\")\n",
        "        print(f\"  Error: {abs(gt_percentage - pred_percentage):.2f}%\")\n",
        "    else:\n",
        "        print(f\"  Prediction: {pred_percentage:.2f}% affected\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "inference_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Upload Your Own Image for Testing"
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your own fundus image for testing\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"Upload a fundus image to test:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Processing uploaded file: {filename}\")\n",
        "    \n",
        "    # Save uploaded file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(uploaded[filename])\n",
        "    \n",
        "    # Make prediction\n",
        "    try:\n",
        "        original, pred_mask, pred_percentage = predict_fundus_image(\n",
        "            model, filename, DEVICE, IMAGE_SIZE\n",
        "        )\n",
        "        \n",
        "        # Visualize\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        \n",
        "        axes[0].imshow(original)\n",
        "        axes[0].set_title('Original Image')\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        axes[1].imshow(pred_mask, cmap='gray')\n",
        "        axes[1].set_title(f'Prediction\\n({pred_percentage:.1f}% affected)')\n",
        "        axes[1].axis('off')\n",
        "        \n",
        "        # Overlay\n",
        "        overlay = original.copy()\n",
        "        colored_mask = np.zeros_like(overlay)\n",
        "        colored_mask[pred_mask > 0.5] = [255, 0, 0]\n",
        "        combined = cv2.addWeighted(overlay, 0.7, colored_mask, 0.3, 0)\n",
        "        \n",
        "        axes[2].imshow(combined)\n",
        "        axes[2].set_title('Overlay (Red = Affected)')\n",
        "        axes[2].axis('off')\n",
        "        \n",
        "        plt.suptitle(f'Fundus Analysis: {filename}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"Analysis complete for {filename}:\")\n",
        "        print(f\"  Affected percentage: {pred_percentage:.2f}%\")\n",
        "        \n",
        "        # Provide interpretation\n",
        "        if pred_percentage < 1:\n",
        "            print(f\"  Interpretation: Minimal or no visible pathology\")\n",
        "        elif pred_percentage < 5:\n",
        "            print(f\"  Interpretation: Mild pathological changes detected\")\n",
        "        elif pred_percentage < 15:\n",
        "            print(f\"  Interpretation: Moderate pathological changes\")\n",
        "        else:\n",
        "            print(f\"  Interpretation: Significant pathological changes detected\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")"
      ],
      "metadata": {
        "id": "upload_test_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Save and Download Model"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model for download\n",
        "final_model_path = 'fundus_segmentation_model.pth'\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': {\n",
        "        'n_channels': 3,\n",
        "        'n_classes': 2,\n",
        "        'bilinear': True,\n",
        "        'image_size': IMAGE_SIZE\n",
        "    },\n",
        "    'training_config': {\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'num_epochs': NUM_EPOCHS\n",
        "    },\n",
        "    'performance': summary if 'summary' in locals() else None\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"Model saved as: {final_model_path}\")\n",
        "print(f\"Model size: {os.path.getsize(final_model_path) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Create a simple usage script\n",
        "usage_script = '''\n",
        "# Simple usage script for the trained model\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"Load the trained model\"\"\"\n",
        "    # Define model architecture (copy from your implementation)\n",
        "    from models import UNet\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model = UNet(**checkpoint['model_config'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def predict_percentage(model, image_path, threshold=0.5):\n",
        "    \"\"\"Predict affected percentage for a fundus image\"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (512, 512))\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    \n",
        "    # Convert to tensor\n",
        "    image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).unsqueeze(0)\n",
        "    \n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        pred_mask = (probs[0, 1, :, :] > threshold).float()\n",
        "        percentage = (pred_mask.sum() / pred_mask.numel() * 100).item()\n",
        "    \n",
        "    return percentage\n",
        "\n",
        "# Usage example:\n",
        "# model = load_model('fundus_segmentation_model.pth')\n",
        "# percentage = predict_percentage(model, 'your_fundus_image.jpg')\n",
        "# print(f\"Affected percentage: {percentage:.2f}%\")\n",
        "'''\n",
        "\n",
        "with open('usage_example.py', 'w') as f:\n",
        "    f.write(usage_script)\n",
        "\n",
        "print(\"Usage script created: usage_example.py\")\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download(final_model_path)\n",
        "files.download('usage_example.py')\n",
        "\n",
        "print(\"Files ready for download!\")"
      ],
      "metadata": {
        "id": "save_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Summary and Next Steps\n",
        "\n",
        "### What we accomplished:\n",
        "1. ✅ Implemented U-Net architecture for fundus image segmentation\n",
        "2. ✅ Created comprehensive training pipeline with data augmentation\n",
        "3. ✅ Implemented combined loss function (BCE + Dice)\n",
        "4. ✅ Added detailed evaluation metrics\n",
        "5. ✅ Created percentage calculation for affected areas\n",
        "6. ✅ Provided inference capabilities for new images\n",
        "\n",
        "### Model Performance:\n",
        "- The model can predict the percentage of affected areas in fundus images\n",
        "- Uses state-of-the-art U-Net architecture with proper augmentation\n",
        "- Includes comprehensive evaluation metrics (IoU, Dice, accuracy, sensitivity, specificity)\n",
        "\n",
        "### To improve accuracy further:\n",
        "1. **Use real medical datasets**: DRIVE, STARE, IDRiD, or other clinical datasets\n",
        "2. **Increase training data**: More diverse, high-quality annotated images\n",
        "3. **Advanced architectures**: Try U-Net++, DeepLabV3+, or Vision Transformers\n",
        "4. **Ensemble methods**: Combine multiple models for better predictions\n",
        "5. **Domain adaptation**: Fine-tune on specific pathology types\n",
        "6. **Advanced preprocessing**: Implement vessel enhancement, contrast normalization\n",
        "\n",
        "### References implemented:\n",
        "- **U-Net architecture** based on https://arxiv.org/abs/1610.02391\n",
        "- **Medical image analysis concepts** from JAMA paper\n",
        "- **Best practices** for fundus image processing\n",
        "\n",
        "The model is ready for use and can be further improved with real clinical data!"
      ],
      "metadata": {
        "id": "summary_cell"
      }
    }
  ]
}